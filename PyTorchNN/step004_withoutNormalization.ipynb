{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import ndimage, signal\n",
    "from scipy import misc\n",
    "import skimage.transform \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp/step003_architecture\n"
     ]
    }
   ],
   "source": [
    "device ='cpu'\n",
    "supplDevice = 'cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:1'\n",
    "\n",
    "base_learning_rate = 0.005\n",
    "batch_size = 1000\n",
    "total_epoch_num = 200\n",
    "\n",
    "exp_dir = './exp' # experiment directory, used for reading the init model\n",
    "project_name = 'step004_withoutNormalization'\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir = os.path.join(exp_dir, project_name)\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBattack(Dataset):\n",
    "    def __init__(self, root_dir='DB_trainval.pkl', set_name='train'):\n",
    "        self.root_dir = root_dir\n",
    "        with open(self.root_dir, 'rb') as handle:\n",
    "            self.DB = pickle.load(handle) \n",
    "        #self.itemX = DB['trainX']\n",
    "        #self.itemY = DB['trainY']\n",
    "        self.set_name = set_name # train val/test\n",
    "        self.set_index = {}\n",
    "        self.set_index['train'] = list(range(6637186))\n",
    "        self.set_index['val'] = list(range(6637186, len(self.DB['trainY'])))\n",
    "        self.current_set_len = len(self.set_index[self.set_name])   \n",
    "        \n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.current_set_len\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        curIndex = idx\n",
    "        if self.set_name=='val':\n",
    "            curIndex += 6637186\n",
    "            \n",
    "        X = self.DB['trainX'][curIndex]\n",
    "        Y = self.DB['trainY'][curIndex]\n",
    "        Y = np.asarray([Y]).astype(np.float32)\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        Y = torch.tensor([Y])\n",
    "        return X, Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 6637186, 'val': 500000}\n"
     ]
    }
   ],
   "source": [
    "whole_datasets = {set_name: \n",
    "                  DBattack(root_dir='DB_trainval_withoutNormalization.pkl',\n",
    "                           set_name=set_name)\n",
    "                  for set_name in ['train', 'val']}\n",
    "\n",
    "\n",
    "dataloaders = {set_name: DataLoader(whole_datasets[set_name], \n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=set_name=='train', \n",
    "                                    num_workers=2) # num_work can be set to batch_size\n",
    "               for set_name in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {set_name: len(whole_datasets[set_name]) for set_name in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(model, dataloaders, dataset_sizes, lossFunc, optimizer, scheduler, \n",
    "             num_epochs=125, work_dir='./', device='cpu'):\n",
    "    \n",
    "    log_filename = os.path.join(work_dir,'train.log')    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):        \n",
    "        print('\\nEpoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        fn = open(log_filename,'a')\n",
    "        fn.write('\\nEpoch {}/{}\\n'.format(epoch+1, num_epochs))\n",
    "        fn.write('--'*5+'\\n')\n",
    "        fn.close()\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print(phase)\n",
    "            fn = open(log_filename,'a')        \n",
    "            fn.write(phase+'\\n')\n",
    "            fn.close()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else: \n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            # Iterate over data.\n",
    "            iterCount, sampleCount = 0, 0\n",
    "            for sample in dataloaders[phase]:\n",
    "                Xlist, ylist = sample                                \n",
    "                Xlist = Xlist.to(device)\n",
    "                ylist = ylist.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    if phase=='train':  # backward + optimize only if in training phase\n",
    "                        model.train()\n",
    "                        outputs = model(Xlist)\n",
    "                        predicted = outputs.detach()>0.5\n",
    "                        predicted = predicted.type(torch.float)\n",
    "                        acc = (predicted-ylist)==0\n",
    "                        acc = acc.type(torch.float).mean()\n",
    "\n",
    "                        loss = lossFunc(outputs, ylist)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else: \n",
    "                        model.eval()  \n",
    "                        outputs = model(Xlist)   \n",
    "                        predicted = outputs.detach()>0.5\n",
    "                        predicted = predicted.type(torch.float)\n",
    "                        acc = predicted-ylist\n",
    "                        acc = (predicted-ylist)==0\n",
    "                        acc = acc.type(torch.float).mean()\n",
    "                        \n",
    "                        loss = lossFunc(outputs, ylist)\n",
    "\n",
    "                # statistics  \n",
    "                iterCount += 1\n",
    "                sampleCount += ylist.size(0)\n",
    "                running_acc += acc.item() * ylist.size(0) \n",
    "                running_loss += loss.item() * ylist.size(0)                                \n",
    "                print2screen_avgLoss = running_loss/sampleCount                          \n",
    "                print2screen_avgAcc = running_acc/sampleCount\n",
    "                                               \n",
    "                if iterCount%200==0:\n",
    "                    print(\n",
    "                        '\\t{}/{} loss: {:.6f}   acc: {:.5f}'.format(\n",
    "                            iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))\n",
    "                    fn = open(log_filename,'a')        \n",
    "                    fn.write('\\t{}/{} loss: {:.6f}   acc: {:.5f}\\n'.format(iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))\n",
    "                    fn.close()\n",
    "  \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                                \n",
    "            print('\\tloss: {:.6f}'.format(epoch_loss))\n",
    "            fn = open(log_filename,'a')\n",
    "            fn.write('\\tloss: {:.6f}\\n'.format(epoch_loss))\n",
    "            fn.close()\n",
    "                    \n",
    "                \n",
    "            # deep copy the model\n",
    "            cur_model_wts = copy.deepcopy(model.state_dict())\n",
    "            path_to_save_paramOnly = os.path.join(work_dir, 'epoch-{}.paramOnly'.format(epoch+1))\n",
    "            torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "            \n",
    "            if phase=='val' and epoch_loss<best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                path_to_save_paramOnly = os.path.join(work_dir, 'bestValModel.paramOnly')\n",
    "                torch.save(best_model_wts, path_to_save_paramOnly)\n",
    "                #path_to_save_wholeModel = os.path.join(work_dir, 'bestValModel.wholeModel')\n",
    "                #torch.save(model, path_to_save_wholeModel)\n",
    "                \n",
    "                file_to_note_bestModel = os.path.join(work_dir,'note_bestModel.log')\n",
    "                fn = open(file_to_note_bestModel,'a')\n",
    "                fn.write('The best model is achieved at epoch-{}: loss{:.6f}.\\n'.format(epoch+1,best_loss))\n",
    "                fn.close()\n",
    "                \n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    fn = open(log_filename,'a')\n",
    "    fn.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    fn.close()\n",
    "   \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=56, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(56, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc_final = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc_final(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=base_learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=int(total_epoch_num/3), gamma=0.5)\n",
    "lossFunc = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "----------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/6638 loss: 0.692636   acc: 0.50096\n",
      "\t400/6638 loss: 0.692204   acc: 0.50051\n",
      "\t600/6638 loss: 0.691673   acc: 0.50033\n",
      "\t800/6638 loss: 0.690890   acc: 0.50021\n",
      "\t1000/6638 loss: 0.689813   acc: 0.50024\n",
      "\t1200/6638 loss: 0.688495   acc: 0.50013\n",
      "\t1400/6638 loss: 0.687015   acc: 0.50013\n",
      "\t1600/6638 loss: 0.685610   acc: 0.50017\n",
      "\t1800/6638 loss: 0.684409   acc: 0.50013\n",
      "\t2000/6638 loss: 0.683327   acc: 0.50013\n",
      "\t2200/6638 loss: 0.682299   acc: 0.50012\n",
      "\t2400/6638 loss: 0.681431   acc: 0.50010\n",
      "\t2600/6638 loss: 0.680509   acc: 0.50009\n",
      "\t2800/6638 loss: 0.679710   acc: 0.50010\n",
      "\t3000/6638 loss: 0.679006   acc: 0.50010\n",
      "\t3200/6638 loss: 0.678283   acc: 0.50009\n",
      "\t3400/6638 loss: 0.677628   acc: 0.50009\n",
      "\t3600/6638 loss: 0.677094   acc: 0.50009\n",
      "\t3800/6638 loss: 0.676519   acc: 0.50010\n",
      "\t4000/6638 loss: 0.676010   acc: 0.50009\n",
      "\t4200/6638 loss: 0.675538   acc: 0.50009\n",
      "\t4400/6638 loss: 0.675089   acc: 0.50009\n",
      "\t4600/6638 loss: 0.674681   acc: 0.50009\n",
      "\t4800/6638 loss: 0.674288   acc: 0.50008\n",
      "\t5000/6638 loss: 0.673960   acc: 0.50009\n",
      "\t5200/6638 loss: 0.673661   acc: 0.50009\n",
      "\t5400/6638 loss: 0.673336   acc: 0.50009\n",
      "\t5600/6638 loss: 0.673030   acc: 0.50009\n",
      "\t5800/6638 loss: 0.672733   acc: 0.50009\n",
      "\t6000/6638 loss: 0.672438   acc: 0.50009\n",
      "\t6200/6638 loss: 0.672172   acc: 0.50009\n",
      "\t6400/6638 loss: 0.671916   acc: 0.50009\n",
      "\t6600/6638 loss: 0.671639   acc: 0.50009\n",
      "\tloss: 0.671598\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/500 loss: 0.663070   acc: 0.50006\n",
      "\t400/500 loss: 0.662696   acc: 0.50008\n",
      "\tloss: 0.662637\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/6638 loss: 0.663446   acc: 0.50017\n",
      "\t400/6638 loss: 0.663485   acc: 0.50013\n",
      "\t600/6638 loss: 0.663162   acc: 0.50010\n",
      "\t800/6638 loss: 0.662862   acc: 0.50008\n",
      "\t1000/6638 loss: 0.663029   acc: 0.50006\n",
      "\t1200/6638 loss: 0.662928   acc: 0.50009\n",
      "\t1400/6638 loss: 0.662892   acc: 0.50008\n",
      "\t1600/6638 loss: 0.662795   acc: 0.50009\n",
      "\t1800/6638 loss: 0.662763   acc: 0.50008\n",
      "\t2000/6638 loss: 0.662686   acc: 0.50007\n",
      "\t2200/6638 loss: 0.662563   acc: 0.50007\n",
      "\t2400/6638 loss: 0.662531   acc: 0.50006\n",
      "\t2600/6638 loss: 0.662411   acc: 0.50008\n",
      "\t2800/6638 loss: 0.662320   acc: 0.50008\n",
      "\t3000/6638 loss: 0.662234   acc: 0.50008\n",
      "\t3200/6638 loss: 0.662171   acc: 0.50009\n",
      "\t3400/6638 loss: 0.662101   acc: 0.50009\n",
      "\t3600/6638 loss: 0.662031   acc: 0.50009\n",
      "\t3800/6638 loss: 0.661982   acc: 0.50010\n",
      "\t4000/6638 loss: 0.661922   acc: 0.50010\n",
      "\t4200/6638 loss: 0.661854   acc: 0.50010\n",
      "\t4400/6638 loss: 0.661784   acc: 0.50010\n",
      "\t4600/6638 loss: 0.661735   acc: 0.50009\n",
      "\t4800/6638 loss: 0.661648   acc: 0.50009\n",
      "\t5000/6638 loss: 0.661578   acc: 0.50010\n",
      "\t5200/6638 loss: 0.661534   acc: 0.50010\n",
      "\t5400/6638 loss: 0.661423   acc: 0.50010\n",
      "\t5600/6638 loss: 0.661325   acc: 0.50010\n",
      "\t5800/6638 loss: 0.661262   acc: 0.50010\n",
      "\t6000/6638 loss: 0.661221   acc: 0.50011\n",
      "\t6200/6638 loss: 0.661153   acc: 0.50010\n",
      "\t6400/6638 loss: 0.661101   acc: 0.50010\n",
      "\t6600/6638 loss: 0.661040   acc: 0.50010\n",
      "\tloss: 0.661024\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/500 loss: 0.658142   acc: 0.50005\n",
      "\t400/500 loss: 0.657602   acc: 0.50010\n",
      "\tloss: 0.657508\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/6638 loss: 0.659316   acc: 0.50001\n",
      "\t400/6638 loss: 0.658912   acc: 0.50002\n",
      "\t600/6638 loss: 0.658939   acc: 0.50006\n",
      "\t800/6638 loss: 0.658848   acc: 0.50010\n",
      "\t1000/6638 loss: 0.658932   acc: 0.50009\n",
      "\t1200/6638 loss: 0.658869   acc: 0.50010\n",
      "\t1400/6638 loss: 0.658766   acc: 0.50009\n",
      "\t1600/6638 loss: 0.658518   acc: 0.50010\n",
      "\t1800/6638 loss: 0.658352   acc: 0.50012\n",
      "\t2000/6638 loss: 0.658303   acc: 0.50011\n",
      "\t2200/6638 loss: 0.658195   acc: 0.50010\n",
      "\t2400/6638 loss: 0.658129   acc: 0.50010\n",
      "\t2600/6638 loss: 0.658076   acc: 0.50010\n",
      "\t2800/6638 loss: 0.657994   acc: 0.50011\n",
      "\t3000/6638 loss: 0.658044   acc: 0.50010\n",
      "\t3200/6638 loss: 0.657958   acc: 0.50010\n",
      "\t3400/6638 loss: 0.657887   acc: 0.50010\n",
      "\t3600/6638 loss: 0.657774   acc: 0.50010\n",
      "\t3800/6638 loss: 0.657704   acc: 0.50010\n",
      "\t4000/6638 loss: 0.657611   acc: 0.50010\n",
      "\t4200/6638 loss: 0.657539   acc: 0.50010\n",
      "\t4400/6638 loss: 0.657434   acc: 0.50011\n",
      "\t4600/6638 loss: 0.657401   acc: 0.50011\n",
      "\t4800/6638 loss: 0.657336   acc: 0.50010\n",
      "\t5000/6638 loss: 0.657212   acc: 0.50010\n",
      "\t5200/6638 loss: 0.657119   acc: 0.50011\n",
      "\t5400/6638 loss: 0.657075   acc: 0.50011\n",
      "\t5600/6638 loss: 0.657010   acc: 0.50011\n",
      "\t5800/6638 loss: 0.656953   acc: 0.50011\n",
      "\t6000/6638 loss: 0.656877   acc: 0.50011\n",
      "\t6200/6638 loss: 0.656822   acc: 0.50012\n",
      "\t6400/6638 loss: 0.656711   acc: 0.50011\n",
      "\t6600/6638 loss: 0.656631   acc: 0.50012\n",
      "\tloss: 0.656607\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/500 loss: 0.653998   acc: 0.50000\n",
      "\t400/500 loss: 0.653412   acc: 0.50010\n",
      "\tloss: 0.653292\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72fa110390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/skong2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/6638 loss: 0.654607   acc: 0.50013\n",
      "\t400/6638 loss: 0.654000   acc: 0.50010\n",
      "\t600/6638 loss: 0.653905   acc: 0.50009\n",
      "\t800/6638 loss: 0.653909   acc: 0.50008\n",
      "\t1000/6638 loss: 0.653784   acc: 0.50009\n"
     ]
    }
   ],
   "source": [
    "model_best = trainval(model, dataloaders, dataset_sizes, \n",
    "                       lossFunc, \n",
    "                       optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=total_epoch_num, \n",
    "                       work_dir=save_dir, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaving blank (backup)\n",
    "\n",
    "    1. dataloader, batch size\n",
    "    2. network architecture\n",
    "    3. optimizer\n",
    "    4. loss functioin, binary classification, BCEloss\n",
    "    5. training function\n",
    "    6. evaluation\n",
    "    \n",
    "    \n",
    "jupyter nbconvert --to script step001_prepare_dataset.ipynb    \n",
    "nohup python step001_prepare_dataset.py 1>step001_prepare_dataset.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
