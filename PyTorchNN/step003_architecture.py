#!/usr/bin/env python
# coding: utf-8

# In[53]:


import torch
import pickle
import numpy as np

import os, random, time, copy
import numpy as np
import os.path as path
import scipy.io as sio
from scipy import ndimage, signal
from scipy import misc
import skimage.transform 
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler 
import torch.nn.functional as F
from torch.autograd import Variable
import torchvision
from torchvision import datasets, models, transforms


# In[60]:


device ='cpu'
supplDevice = 'cpu'
if torch.cuda.is_available(): 
    device='cuda:0'

base_learning_rate = 0.005
batch_size = 1000
total_epoch_num = 200

exp_dir = './exp' # experiment directory, used for reading the init model
project_name = 'step003_architecture'

torch.cuda.device_count()
torch.cuda.empty_cache()

save_dir = os.path.join(exp_dir, project_name)
print(save_dir)    
if not os.path.exists(save_dir): os.makedirs(save_dir)

log_filename = os.path.join(save_dir, 'train.log')


# In[61]:


class DBattack(Dataset):
    def __init__(self, root_dir='DB_trainval.pkl', set_name='train'):
        self.root_dir = root_dir
        with open(self.root_dir, 'rb') as handle:
            self.DB = pickle.load(handle) 
        #self.itemX = DB['trainX']
        #self.itemY = DB['trainY']
        self.set_name = set_name # train val/test
        self.set_index = {}
        self.set_index['train'] = list(range(6637186))
        self.set_index['val'] = list(range(6637186, len(self.DB['trainY'])))
        self.current_set_len = len(self.set_index[self.set_name])   
        
        
    def __len__(self):        
        return self.current_set_len
    
    
    def __getitem__(self, idx):
        curIndex = idx
        if self.set_name=='val':
            curIndex += 6637186
            
        X = self.DB['trainX'][curIndex]
        Y = self.DB['trainY'][curIndex]
        Y = np.asarray([Y]).astype(np.float32)
        X = torch.from_numpy(X.astype(np.float32))
        Y = torch.tensor([Y])
        return X, Y  


# In[62]:


whole_datasets = {set_name: 
                  DBattack(root_dir='DB_trainval.pkl',
                           set_name=set_name)
                  for set_name in ['train', 'val']}


dataloaders = {set_name: DataLoader(whole_datasets[set_name], 
                                    batch_size=batch_size,
                                    shuffle=set_name=='train', 
                                    num_workers=2) # num_work can be set to batch_size
               for set_name in ['train', 'val']}

dataset_sizes = {set_name: len(whole_datasets[set_name]) for set_name in ['train', 'val']}
print(dataset_sizes)


# In[69]:


def trainval(model, dataloaders, dataset_sizes, lossFunc, optimizer, scheduler, 
             num_epochs=125, work_dir='./', device='cpu'):
    
    log_filename = os.path.join(work_dir,'train.log')    
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = float('inf')

    for epoch in range(num_epochs):        
        print('\nEpoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)
        fn = open(log_filename,'a')
        fn.write('\nEpoch {}/{}\n'.format(epoch+1, num_epochs))
        fn.write('--'*5+'\n')
        fn.close()
        
        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            print(phase)
            fn = open(log_filename,'a')        
            fn.write(phase+'\n')
            fn.close()
            
            if phase == 'train':
                scheduler.step()
                model.train()  # Set model to training mode
            else: 
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_acc = 0.0
            # Iterate over data.
            iterCount, sampleCount = 0, 0
            for sample in dataloaders[phase]:
                Xlist, ylist = sample                                
                Xlist = Xlist.to(device)
                ylist = ylist.to(device)
                
                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train                
                with torch.set_grad_enabled(phase=='train'):
                    if phase=='train':  # backward + optimize only if in training phase
                        model.train()
                        outputs = model(Xlist)
                        predicted = outputs.detach()>0.5
                        predicted = predicted.type(torch.float)
                        acc = (predicted-ylist)==0
                        acc = acc.type(torch.float).mean()

                        loss = lossFunc(outputs, ylist)
                        loss.backward()
                        optimizer.step()
                    else: 
                        model.eval()  
                        outputs = model(Xlist)   
                        predicted = outputs.detach()>0.5
                        predicted = predicted.type(torch.float)
                        acc = predicted-ylist
                        acc = (predicted-ylist)==0
                        acc = acc.type(torch.float).mean()
                        
                        loss = lossFunc(outputs, ylist)

                # statistics  
                iterCount += 1
                sampleCount += ylist.size(0)
                running_acc += acc.item() * ylist.size(0) 
                running_loss += loss.item() * ylist.size(0)                                
                print2screen_avgLoss = running_loss/sampleCount                          
                print2screen_avgAcc = running_acc/sampleCount
                                               
                if iterCount%200==0:
                    print(
                        '\t{}/{} loss: {:.6f}   acc: {:.5f}'.format(
                            iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))
                    fn = open(log_filename,'a')        
                    fn.write('\t{}/{} loss: {:.6f}   acc: {:.5f}\n'.format(iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))
                    fn.close()
  
            epoch_loss = running_loss / dataset_sizes[phase]
                                
            print('\tloss: {:.6f}'.format(epoch_loss))
            fn = open(log_filename,'a')
            fn.write('\tloss: {:.6f}\n'.format(epoch_loss))
            fn.close()
                    
                
            # deep copy the model
            cur_model_wts = copy.deepcopy(model.state_dict())
            path_to_save_paramOnly = os.path.join(work_dir, 'epoch-{}.paramOnly'.format(epoch+1))
            torch.save(cur_model_wts, path_to_save_paramOnly)
            
            if phase=='val' and epoch_loss<best_loss:
                best_loss = epoch_loss
                best_model_wts = copy.deepcopy(model.state_dict())
                
                path_to_save_paramOnly = os.path.join(work_dir, 'bestValModel.paramOnly')
                torch.save(best_model_wts, path_to_save_paramOnly)
                #path_to_save_wholeModel = os.path.join(work_dir, 'bestValModel.wholeModel')
                #torch.save(model, path_to_save_wholeModel)
                
                file_to_note_bestModel = os.path.join(work_dir,'note_bestModel.log')
                fn = open(file_to_note_bestModel,'a')
                fn.write('The best model is achieved at epoch-{}: loss{:.6f}.\n'.format(epoch+1,best_loss))
                fn.close()
                
                
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    
    fn = open(log_filename,'a')
    fn.write('Training complete in {:.0f}m {:.0f}s\n'.format(time_elapsed // 60, time_elapsed % 60))
    fn.close()
   
    # load best model weights
    model.load_state_dict(best_model_wts)
    return model


# In[70]:


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(56, 128)  # 6*6 from image dimension
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 128)
        self.fc_final = nn.Linear(128, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)        
        x = self.fc_final(x)
        x = self.sigmoid(x)
        return x
    
model = Net()
model.to(device)
print(model)

optimizer = optim.SGD(model.parameters(), lr=base_learning_rate, momentum=0.9)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=int(total_epoch_num/3), gamma=0.5)
lossFunc = nn.BCELoss()


# In[ ]:


model_best = trainval(model, dataloaders, dataset_sizes, 
                       lossFunc, 
                       optimizer, exp_lr_scheduler,
                       num_epochs=total_epoch_num, 
                       work_dir=save_dir, device=device)


# In[ ]:





# In[ ]:





# # leaving blank (backup)
# 
#     1. dataloader, batch size
#     2. network architecture
#     3. optimizer
#     4. loss functioin, binary classification, BCEloss
#     5. training function
#     6. evaluation
#     
#     
# jupyter nbconvert --to script step001_prepare_dataset.ipynb    
# nohup python step001_prepare_dataset.py 1>step001_prepare_dataset.log &

# In[ ]:





# In[ ]:





# In[ ]:




