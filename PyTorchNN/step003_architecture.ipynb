{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import ndimage, signal\n",
    "from scipy import misc\n",
    "import skimage.transform \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp/step003_architecture\n"
     ]
    }
   ],
   "source": [
    "device ='cpu'\n",
    "supplDevice = 'cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:1'\n",
    "\n",
    "base_learning_rate = 0.005\n",
    "batch_size = 1000\n",
    "total_epoch_num = 200\n",
    "\n",
    "exp_dir = './exp' # experiment directory, used for reading the init model\n",
    "project_name = 'step003_architecture'\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir = os.path.join(exp_dir, project_name)\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBattack(Dataset):\n",
    "    def __init__(self, root_dir='DB_trainval.pkl', set_name='train'):\n",
    "        self.root_dir = root_dir\n",
    "        with open(self.root_dir, 'rb') as handle:\n",
    "            self.DB = pickle.load(handle) \n",
    "        #self.itemX = DB['trainX']\n",
    "        #self.itemY = DB['trainY']\n",
    "        self.set_name = set_name # train val/test\n",
    "        self.set_index = {}\n",
    "        self.set_index['train'] = list(range(6637186))\n",
    "        self.set_index['val'] = list(range(6637186, len(self.DB['trainY'])))\n",
    "        self.current_set_len = len(self.set_index[self.set_name])   \n",
    "        \n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.current_set_len\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        curIndex = idx\n",
    "        if self.set_name=='val':\n",
    "            curIndex += 6637186\n",
    "            \n",
    "        X = self.DB['trainX'][curIndex]\n",
    "        Y = self.DB['trainY'][curIndex]\n",
    "        Y = np.asarray([Y]).astype(np.float32)\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        Y = torch.tensor([Y])\n",
    "        return X, Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 6637186, 'val': 500000}\n"
     ]
    }
   ],
   "source": [
    "whole_datasets = {set_name: \n",
    "                  DBattack(root_dir='DB_trainval.pkl',\n",
    "                           set_name=set_name)\n",
    "                  for set_name in ['train', 'val']}\n",
    "\n",
    "\n",
    "dataloaders = {set_name: DataLoader(whole_datasets[set_name], \n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=set_name=='train', \n",
    "                                    num_workers=2) # num_work can be set to batch_size\n",
    "               for set_name in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {set_name: len(whole_datasets[set_name]) for set_name in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(model, dataloaders, dataset_sizes, lossFunc, optimizer, scheduler, \n",
    "             num_epochs=125, work_dir='./', device='cpu'):\n",
    "    \n",
    "    log_filename = os.path.join(work_dir,'train.log')    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):        \n",
    "        print('\\nEpoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        fn = open(log_filename,'a')\n",
    "        fn.write('\\nEpoch {}/{}\\n'.format(epoch+1, num_epochs))\n",
    "        fn.write('--'*5+'\\n')\n",
    "        fn.close()\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print(phase)\n",
    "            fn = open(log_filename,'a')        \n",
    "            fn.write(phase+'\\n')\n",
    "            fn.close()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else: \n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            # Iterate over data.\n",
    "            iterCount, sampleCount = 0, 0\n",
    "            for sample in dataloaders[phase]:\n",
    "                Xlist, ylist = sample                                \n",
    "                Xlist = Xlist.to(device)\n",
    "                ylist = ylist.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    if phase=='train':  # backward + optimize only if in training phase\n",
    "                        model.train()\n",
    "                        outputs = model(Xlist)\n",
    "                        predicted = outputs.detach()>0.5\n",
    "                        predicted = predicted.type(torch.float)\n",
    "                        acc = (predicted-ylist)==0\n",
    "                        acc = acc.type(torch.float).mean()\n",
    "\n",
    "                        loss = lossFunc(outputs, ylist)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else: \n",
    "                        model.eval()  \n",
    "                        outputs = model(Xlist)   \n",
    "                        predicted = outputs.detach()>0.5\n",
    "                        predicted = predicted.type(torch.float)\n",
    "                        acc = predicted-ylist\n",
    "                        acc = (predicted-ylist)==0\n",
    "                        acc = acc.type(torch.float).mean()\n",
    "                        \n",
    "                        loss = lossFunc(outputs, ylist)\n",
    "\n",
    "                # statistics  \n",
    "                iterCount += 1\n",
    "                sampleCount += ylist.size(0)\n",
    "                running_acc += acc.item() * ylist.size(0) \n",
    "                running_loss += loss.item() * ylist.size(0)                                \n",
    "                print2screen_avgLoss = running_loss/sampleCount                          \n",
    "                print2screen_avgAcc = running_acc/sampleCount\n",
    "                                               \n",
    "                if iterCount%200==0:\n",
    "                    print(\n",
    "                        '\\t{}/{} loss: {:.6f}   acc: {:.5f}'.format(\n",
    "                            iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))\n",
    "                    fn = open(log_filename,'a')        \n",
    "                    fn.write('\\t{}/{} loss: {:.6f}   acc: {:.5f}\\n'.format(iterCount, len(dataloaders[phase]), print2screen_avgLoss, print2screen_avgAcc))\n",
    "                    fn.close()\n",
    "  \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                                \n",
    "            print('\\tloss: {:.6f}'.format(epoch_loss))\n",
    "            fn = open(log_filename,'a')\n",
    "            fn.write('\\tloss: {:.6f}\\n'.format(epoch_loss))\n",
    "            fn.close()\n",
    "                    \n",
    "                \n",
    "            # deep copy the model\n",
    "            cur_model_wts = copy.deepcopy(model.state_dict())\n",
    "            path_to_save_paramOnly = os.path.join(work_dir, 'epoch-{}.paramOnly'.format(epoch+1))\n",
    "            torch.save(cur_model_wts, path_to_save_paramOnly)\n",
    "            \n",
    "            if phase=='val' and epoch_loss<best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                path_to_save_paramOnly = os.path.join(work_dir, 'bestValModel.paramOnly')\n",
    "                torch.save(best_model_wts, path_to_save_paramOnly)\n",
    "                #path_to_save_wholeModel = os.path.join(work_dir, 'bestValModel.wholeModel')\n",
    "                #torch.save(model, path_to_save_wholeModel)\n",
    "                \n",
    "                file_to_note_bestModel = os.path.join(work_dir,'note_bestModel.log')\n",
    "                fn = open(file_to_note_bestModel,'a')\n",
    "                fn.write('The best model is achieved at epoch-{}: loss{:.6f}.\\n'.format(epoch+1,best_loss))\n",
    "                fn.close()\n",
    "                \n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    fn = open(log_filename,'a')\n",
    "    fn.write('Training complete in {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    fn.close()\n",
    "   \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=56, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc_final): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(56, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc_final = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc_final(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=base_learning_rate, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=int(total_epoch_num/3), gamma=0.5)\n",
    "lossFunc = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "----------\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patton/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/patton/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1000, 1, 1])) that is different to the input size (torch.Size([1000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/6638 loss: 0.693060   acc: 0.50001\n",
      "\t400/6638 loss: 0.692767   acc: 0.49999\n",
      "\t600/6638 loss: 0.692449   acc: 0.50017\n",
      "\t800/6638 loss: 0.692045   acc: 0.50008\n",
      "\t1000/6638 loss: 0.691479   acc: 0.50008\n",
      "\t1200/6638 loss: 0.690684   acc: 0.50009\n",
      "\t1400/6638 loss: 0.689594   acc: 0.50011\n",
      "\t1600/6638 loss: 0.688311   acc: 0.50005\n",
      "\t1800/6638 loss: 0.687114   acc: 0.50004\n",
      "\t2000/6638 loss: 0.685973   acc: 0.50007\n",
      "\t2200/6638 loss: 0.684825   acc: 0.50007\n",
      "\t2400/6638 loss: 0.683831   acc: 0.50007\n",
      "\t2600/6638 loss: 0.682934   acc: 0.50007\n",
      "\t2800/6638 loss: 0.682100   acc: 0.50007\n",
      "\t3000/6638 loss: 0.681283   acc: 0.50006\n",
      "\t3200/6638 loss: 0.680568   acc: 0.50005\n",
      "\t3400/6638 loss: 0.679810   acc: 0.50006\n",
      "\t3600/6638 loss: 0.679150   acc: 0.50006\n",
      "\t3800/6638 loss: 0.678548   acc: 0.50005\n",
      "\t4000/6638 loss: 0.677946   acc: 0.50005\n",
      "\t4200/6638 loss: 0.677383   acc: 0.50005\n",
      "\t4400/6638 loss: 0.676874   acc: 0.50006\n",
      "\t4600/6638 loss: 0.676397   acc: 0.50006\n",
      "\t4800/6638 loss: 0.675958   acc: 0.50006\n",
      "\t5000/6638 loss: 0.675513   acc: 0.50007\n",
      "\t5200/6638 loss: 0.675115   acc: 0.50007\n",
      "\t5400/6638 loss: 0.674731   acc: 0.50007\n",
      "\t5600/6638 loss: 0.674375   acc: 0.50006\n",
      "\t5800/6638 loss: 0.674024   acc: 0.50006\n",
      "\t6000/6638 loss: 0.673701   acc: 0.50007\n",
      "\t6200/6638 loss: 0.673420   acc: 0.50007\n",
      "\t6400/6638 loss: 0.673128   acc: 0.50006\n",
      "\t6600/6638 loss: 0.672870   acc: 0.50006\n",
      "\tloss: 0.672817\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patton/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([186, 1, 1])) that is different to the input size (torch.Size([186, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t200/500 loss: 0.663090   acc: 0.50008\n",
      "\t400/500 loss: 0.662763   acc: 0.50008\n",
      "\tloss: 0.662699\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.663951   acc: 0.50018\n",
      "\t400/6638 loss: 0.663910   acc: 0.50025\n",
      "\t600/6638 loss: 0.663400   acc: 0.50025\n",
      "\t800/6638 loss: 0.663323   acc: 0.50022\n",
      "\t1000/6638 loss: 0.663220   acc: 0.50018\n",
      "\t1200/6638 loss: 0.663136   acc: 0.50018\n",
      "\t1400/6638 loss: 0.663062   acc: 0.50016\n",
      "\t1600/6638 loss: 0.662989   acc: 0.50015\n",
      "\t1800/6638 loss: 0.662950   acc: 0.50013\n",
      "\t2000/6638 loss: 0.662797   acc: 0.50014\n",
      "\t2200/6638 loss: 0.662751   acc: 0.50014\n",
      "\t2400/6638 loss: 0.662617   acc: 0.50014\n",
      "\t2600/6638 loss: 0.662571   acc: 0.50013\n",
      "\t2800/6638 loss: 0.662452   acc: 0.50013\n",
      "\t3000/6638 loss: 0.662352   acc: 0.50013\n",
      "\t3200/6638 loss: 0.662279   acc: 0.50013\n",
      "\t3400/6638 loss: 0.662218   acc: 0.50014\n",
      "\t3600/6638 loss: 0.662200   acc: 0.50014\n",
      "\t3800/6638 loss: 0.662125   acc: 0.50013\n",
      "\t4000/6638 loss: 0.662075   acc: 0.50013\n",
      "\t4200/6638 loss: 0.662002   acc: 0.50013\n",
      "\t4400/6638 loss: 0.661928   acc: 0.50013\n",
      "\t4600/6638 loss: 0.661873   acc: 0.50013\n",
      "\t4800/6638 loss: 0.661828   acc: 0.50012\n",
      "\t5000/6638 loss: 0.661790   acc: 0.50012\n",
      "\t5200/6638 loss: 0.661715   acc: 0.50012\n",
      "\t5400/6638 loss: 0.661660   acc: 0.50012\n",
      "\t5600/6638 loss: 0.661580   acc: 0.50012\n",
      "\t5800/6638 loss: 0.661512   acc: 0.50012\n",
      "\t6000/6638 loss: 0.661449   acc: 0.50012\n",
      "\t6200/6638 loss: 0.661368   acc: 0.50012\n",
      "\t6400/6638 loss: 0.661290   acc: 0.50013\n",
      "\t6600/6638 loss: 0.661246   acc: 0.50013\n",
      "\tloss: 0.661233\n",
      "val\n",
      "\t200/500 loss: 0.660312   acc: 0.49992\n",
      "\t400/500 loss: 0.659805   acc: 0.50008\n",
      "\tloss: 0.659669\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.659080   acc: 0.50034\n",
      "\t400/6638 loss: 0.659751   acc: 0.50029\n",
      "\t600/6638 loss: 0.659683   acc: 0.50026\n",
      "\t800/6638 loss: 0.659471   acc: 0.50019\n",
      "\t1000/6638 loss: 0.659458   acc: 0.50016\n",
      "\t1200/6638 loss: 0.659438   acc: 0.50014\n",
      "\t1400/6638 loss: 0.659469   acc: 0.50011\n",
      "\t1600/6638 loss: 0.659277   acc: 0.50010\n",
      "\t1800/6638 loss: 0.659253   acc: 0.50013\n",
      "\t2000/6638 loss: 0.659174   acc: 0.50013\n",
      "\t2200/6638 loss: 0.659157   acc: 0.50013\n",
      "\t2400/6638 loss: 0.659083   acc: 0.50013\n",
      "\t2600/6638 loss: 0.658841   acc: 0.50013\n",
      "\t2800/6638 loss: 0.658735   acc: 0.50013\n",
      "\t3000/6638 loss: 0.658698   acc: 0.50014\n",
      "\t3200/6638 loss: 0.658606   acc: 0.50014\n",
      "\t3400/6638 loss: 0.658539   acc: 0.50013\n",
      "\t3600/6638 loss: 0.658537   acc: 0.50011\n",
      "\t3800/6638 loss: 0.658484   acc: 0.50012\n",
      "\t4000/6638 loss: 0.658510   acc: 0.50012\n",
      "\t4200/6638 loss: 0.658424   acc: 0.50011\n",
      "\t4400/6638 loss: 0.658386   acc: 0.50011\n",
      "\t4600/6638 loss: 0.658383   acc: 0.50011\n",
      "\t4800/6638 loss: 0.658354   acc: 0.50010\n",
      "\t5000/6638 loss: 0.658301   acc: 0.50010\n",
      "\t5200/6638 loss: 0.658281   acc: 0.50010\n",
      "\t5400/6638 loss: 0.658210   acc: 0.50010\n",
      "\t5600/6638 loss: 0.658193   acc: 0.50010\n",
      "\t5800/6638 loss: 0.658161   acc: 0.50010\n",
      "\t6000/6638 loss: 0.658096   acc: 0.50010\n",
      "\t6200/6638 loss: 0.658058   acc: 0.50010\n",
      "\t6400/6638 loss: 0.658002   acc: 0.50010\n",
      "\t6600/6638 loss: 0.657978   acc: 0.50010\n",
      "\tloss: 0.657978\n",
      "val\n",
      "\t200/500 loss: 0.656751   acc: 0.49998\n",
      "\t400/500 loss: 0.656252   acc: 0.50009\n",
      "\tloss: 0.656094\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.656139   acc: 0.50014\n",
      "\t400/6638 loss: 0.656656   acc: 0.50017\n",
      "\t600/6638 loss: 0.656687   acc: 0.50015\n",
      "\t800/6638 loss: 0.656926   acc: 0.50014\n",
      "\t1000/6638 loss: 0.656590   acc: 0.50009\n",
      "\t1200/6638 loss: 0.656497   acc: 0.50009\n",
      "\t1400/6638 loss: 0.656559   acc: 0.50008\n",
      "\t1600/6638 loss: 0.656444   acc: 0.50010\n",
      "\t1800/6638 loss: 0.656415   acc: 0.50009\n",
      "\t2000/6638 loss: 0.656343   acc: 0.50008\n",
      "\t2200/6638 loss: 0.656317   acc: 0.50009\n",
      "\t2400/6638 loss: 0.656273   acc: 0.50007\n",
      "\t2600/6638 loss: 0.656201   acc: 0.50006\n",
      "\t2800/6638 loss: 0.656157   acc: 0.50006\n",
      "\t3000/6638 loss: 0.656079   acc: 0.50006\n",
      "\t3200/6638 loss: 0.656060   acc: 0.50007\n",
      "\t3400/6638 loss: 0.656013   acc: 0.50008\n",
      "\t3600/6638 loss: 0.655951   acc: 0.50009\n",
      "\t3800/6638 loss: 0.655945   acc: 0.50009\n",
      "\t4000/6638 loss: 0.655931   acc: 0.50009\n",
      "\t4200/6638 loss: 0.655883   acc: 0.50010\n",
      "\t4400/6638 loss: 0.655804   acc: 0.50009\n",
      "\t4600/6638 loss: 0.655781   acc: 0.50009\n",
      "\t4800/6638 loss: 0.655729   acc: 0.50009\n",
      "\t5000/6638 loss: 0.655707   acc: 0.50009\n",
      "\t5200/6638 loss: 0.655674   acc: 0.50009\n",
      "\t5400/6638 loss: 0.655634   acc: 0.50009\n",
      "\t5600/6638 loss: 0.655624   acc: 0.50008\n",
      "\t5800/6638 loss: 0.655537   acc: 0.50008\n",
      "\t6000/6638 loss: 0.655488   acc: 0.50008\n",
      "\t6200/6638 loss: 0.655453   acc: 0.50008\n",
      "\t6400/6638 loss: 0.655398   acc: 0.50009\n",
      "\t6600/6638 loss: 0.655337   acc: 0.50008\n",
      "\tloss: 0.655349\n",
      "val\n",
      "\t200/500 loss: 0.654989   acc: 0.50023\n",
      "\t400/500 loss: 0.654681   acc: 0.50013\n",
      "\tloss: 0.654569\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.653164   acc: 0.50015\n",
      "\t400/6638 loss: 0.652956   acc: 0.50014\n",
      "\t600/6638 loss: 0.653524   acc: 0.50011\n",
      "\t800/6638 loss: 0.653738   acc: 0.50008\n",
      "\t1000/6638 loss: 0.653798   acc: 0.50010\n",
      "\t1200/6638 loss: 0.653627   acc: 0.50007\n",
      "\t1400/6638 loss: 0.653472   acc: 0.50007\n",
      "\t1600/6638 loss: 0.653423   acc: 0.50010\n",
      "\t1800/6638 loss: 0.653533   acc: 0.50009\n",
      "\t2000/6638 loss: 0.653486   acc: 0.50009\n",
      "\t2200/6638 loss: 0.653374   acc: 0.50009\n",
      "\t2400/6638 loss: 0.653338   acc: 0.50009\n",
      "\t2600/6638 loss: 0.653354   acc: 0.50008\n",
      "\t2800/6638 loss: 0.653350   acc: 0.50007\n",
      "\t3000/6638 loss: 0.653280   acc: 0.50008\n",
      "\t3200/6638 loss: 0.653256   acc: 0.50008\n",
      "\t3400/6638 loss: 0.653204   acc: 0.50009\n",
      "\t3600/6638 loss: 0.653135   acc: 0.50010\n",
      "\t3800/6638 loss: 0.653105   acc: 0.50010\n",
      "\t4000/6638 loss: 0.653100   acc: 0.50009\n",
      "\t4200/6638 loss: 0.653099   acc: 0.50010\n",
      "\t4400/6638 loss: 0.653053   acc: 0.50009\n",
      "\t4600/6638 loss: 0.653032   acc: 0.50008\n",
      "\t4800/6638 loss: 0.653029   acc: 0.50008\n",
      "\t5000/6638 loss: 0.652986   acc: 0.50008\n",
      "\t5200/6638 loss: 0.652963   acc: 0.50007\n",
      "\t5400/6638 loss: 0.652939   acc: 0.50007\n",
      "\t5600/6638 loss: 0.652896   acc: 0.50008\n",
      "\t5800/6638 loss: 0.652884   acc: 0.50008\n",
      "\t6000/6638 loss: 0.652842   acc: 0.50007\n",
      "\t6200/6638 loss: 0.652788   acc: 0.50007\n",
      "\t6400/6638 loss: 0.652724   acc: 0.50007\n",
      "\t6600/6638 loss: 0.652664   acc: 0.50007\n",
      "\tloss: 0.652666\n",
      "val\n",
      "\t200/500 loss: 0.650308   acc: 0.50008\n",
      "\t400/500 loss: 0.650073   acc: 0.50010\n",
      "\tloss: 0.649924\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.650917   acc: 0.50027\n",
      "\t400/6638 loss: 0.650949   acc: 0.50013\n",
      "\t600/6638 loss: 0.650715   acc: 0.50015\n",
      "\t800/6638 loss: 0.650789   acc: 0.50018\n",
      "\t1000/6638 loss: 0.650962   acc: 0.50021\n",
      "\t1200/6638 loss: 0.650916   acc: 0.50020\n",
      "\t1400/6638 loss: 0.650896   acc: 0.50021\n",
      "\t1600/6638 loss: 0.650873   acc: 0.50018\n",
      "\t1800/6638 loss: 0.650753   acc: 0.50016\n",
      "\t2000/6638 loss: 0.650719   acc: 0.50015\n",
      "\t2200/6638 loss: 0.650619   acc: 0.50016\n",
      "\t2400/6638 loss: 0.650520   acc: 0.50017\n",
      "\t2600/6638 loss: 0.650459   acc: 0.50017\n",
      "\t2800/6638 loss: 0.650392   acc: 0.50016\n",
      "\t3000/6638 loss: 0.650377   acc: 0.50015\n",
      "\t3200/6638 loss: 0.650329   acc: 0.50014\n",
      "\t3400/6638 loss: 0.650254   acc: 0.50015\n",
      "\t3600/6638 loss: 0.650146   acc: 0.50015\n",
      "\t3800/6638 loss: 0.650124   acc: 0.50015\n",
      "\t4000/6638 loss: 0.650015   acc: 0.50014\n",
      "\t4200/6638 loss: 0.649905   acc: 0.50014\n",
      "\t4400/6638 loss: 0.649848   acc: 0.50015\n",
      "\t4600/6638 loss: 0.649811   acc: 0.50014\n",
      "\t4800/6638 loss: 0.649699   acc: 0.50014\n",
      "\t5000/6638 loss: 0.649623   acc: 0.50014\n",
      "\t5200/6638 loss: 0.649520   acc: 0.50013\n",
      "\t5400/6638 loss: 0.649443   acc: 0.50013\n",
      "\t5600/6638 loss: 0.649337   acc: 0.50013\n",
      "\t5800/6638 loss: 0.649270   acc: 0.50013\n",
      "\t6000/6638 loss: 0.649187   acc: 0.50014\n",
      "\t6200/6638 loss: 0.649107   acc: 0.50014\n",
      "\t6400/6638 loss: 0.648976   acc: 0.50014\n",
      "\t6600/6638 loss: 0.648873   acc: 0.50014\n",
      "\tloss: 0.648844\n",
      "val\n",
      "\t200/500 loss: 0.644644   acc: 0.50009\n",
      "\t400/500 loss: 0.644339   acc: 0.50010\n",
      "\tloss: 0.644108\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.643870   acc: 0.50011\n",
      "\t400/6638 loss: 0.644715   acc: 0.50022\n",
      "\t600/6638 loss: 0.644472   acc: 0.50016\n",
      "\t800/6638 loss: 0.644583   acc: 0.50023\n",
      "\t1000/6638 loss: 0.644331   acc: 0.50025\n",
      "\t1200/6638 loss: 0.644186   acc: 0.50026\n",
      "\t1400/6638 loss: 0.644078   acc: 0.50026\n",
      "\t1600/6638 loss: 0.644058   acc: 0.50026\n",
      "\t1800/6638 loss: 0.643931   acc: 0.50023\n",
      "\t2000/6638 loss: 0.643785   acc: 0.50022\n",
      "\t2200/6638 loss: 0.643633   acc: 0.50021\n",
      "\t2400/6638 loss: 0.643563   acc: 0.50022\n",
      "\t2600/6638 loss: 0.643446   acc: 0.50020\n",
      "\t2800/6638 loss: 0.643369   acc: 0.50019\n",
      "\t3000/6638 loss: 0.643269   acc: 0.50019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3200/6638 loss: 0.643216   acc: 0.50018\n",
      "\t3400/6638 loss: 0.643098   acc: 0.50019\n",
      "\t3600/6638 loss: 0.642989   acc: 0.50018\n",
      "\t3800/6638 loss: 0.642897   acc: 0.50018\n",
      "\t4000/6638 loss: 0.642780   acc: 0.50017\n",
      "\t4200/6638 loss: 0.642684   acc: 0.50016\n",
      "\t4400/6638 loss: 0.642670   acc: 0.50015\n",
      "\t4600/6638 loss: 0.642551   acc: 0.50015\n",
      "\t4800/6638 loss: 0.642493   acc: 0.50014\n",
      "\t5000/6638 loss: 0.642523   acc: 0.50015\n",
      "\t5200/6638 loss: 0.642451   acc: 0.50016\n",
      "\t5400/6638 loss: 0.642330   acc: 0.50016\n",
      "\t5600/6638 loss: 0.642210   acc: 0.50016\n",
      "\t5800/6638 loss: 0.642193   acc: 0.50016\n",
      "\t6000/6638 loss: 0.642092   acc: 0.50016\n",
      "\t6200/6638 loss: 0.642018   acc: 0.50016\n",
      "\t6400/6638 loss: 0.641954   acc: 0.50015\n",
      "\t6600/6638 loss: 0.641878   acc: 0.50015\n",
      "\tloss: 0.641859\n",
      "val\n",
      "\t200/500 loss: 0.637864   acc: 0.50014\n",
      "\t400/500 loss: 0.637522   acc: 0.50011\n",
      "\tloss: 0.637301\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.639570   acc: 0.50024\n",
      "\t400/6638 loss: 0.639518   acc: 0.50014\n",
      "\t600/6638 loss: 0.639126   acc: 0.50004\n",
      "\t800/6638 loss: 0.638942   acc: 0.50011\n",
      "\t1000/6638 loss: 0.638963   acc: 0.50009\n",
      "\t1200/6638 loss: 0.638994   acc: 0.50008\n",
      "\t1400/6638 loss: 0.639199   acc: 0.50005\n",
      "\t1600/6638 loss: 0.639152   acc: 0.50006\n",
      "\t1800/6638 loss: 0.639220   acc: 0.50008\n",
      "\t2000/6638 loss: 0.639130   acc: 0.50007\n",
      "\t2200/6638 loss: 0.639143   acc: 0.50006\n",
      "\t2400/6638 loss: 0.639143   acc: 0.50007\n",
      "\t2600/6638 loss: 0.639116   acc: 0.50009\n",
      "\t2800/6638 loss: 0.639094   acc: 0.50009\n",
      "\t3000/6638 loss: 0.638990   acc: 0.50009\n",
      "\t3200/6638 loss: 0.639009   acc: 0.50009\n",
      "\t3400/6638 loss: 0.638991   acc: 0.50009\n",
      "\t3600/6638 loss: 0.638906   acc: 0.50008\n",
      "\t3800/6638 loss: 0.638787   acc: 0.50008\n",
      "\t4000/6638 loss: 0.638777   acc: 0.50009\n",
      "\t4200/6638 loss: 0.638759   acc: 0.50008\n",
      "\t4400/6638 loss: 0.638726   acc: 0.50007\n",
      "\t4600/6638 loss: 0.638666   acc: 0.50007\n",
      "\t4800/6638 loss: 0.638609   acc: 0.50008\n",
      "\t5000/6638 loss: 0.638571   acc: 0.50008\n",
      "\t5200/6638 loss: 0.638521   acc: 0.50010\n",
      "\t5400/6638 loss: 0.638487   acc: 0.50010\n",
      "\t5600/6638 loss: 0.638512   acc: 0.50010\n",
      "\t5800/6638 loss: 0.638449   acc: 0.50010\n",
      "\t6000/6638 loss: 0.638435   acc: 0.50011\n",
      "\t6200/6638 loss: 0.638397   acc: 0.50011\n",
      "\t6400/6638 loss: 0.638381   acc: 0.50012\n",
      "\t6600/6638 loss: 0.638334   acc: 0.50012\n",
      "\tloss: 0.638325\n",
      "val\n",
      "\t200/500 loss: 0.638407   acc: 0.50030\n",
      "\t400/500 loss: 0.638221   acc: 0.50012\n",
      "\tloss: 0.637968\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train\n",
      "\t200/6638 loss: 0.639160   acc: 0.50005\n",
      "\t400/6638 loss: 0.638374   acc: 0.50008\n",
      "\t600/6638 loss: 0.637373   acc: 0.50012\n",
      "\t800/6638 loss: 0.637516   acc: 0.50012\n",
      "\t1000/6638 loss: 0.637107   acc: 0.50015\n"
     ]
    }
   ],
   "source": [
    "model_best = trainval(model, dataloaders, dataset_sizes, \n",
    "                       lossFunc, \n",
    "                       optimizer, exp_lr_scheduler,\n",
    "                       num_epochs=total_epoch_num, \n",
    "                       work_dir=save_dir, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaving blank (backup)\n",
    "\n",
    "    1. dataloader, batch size\n",
    "    2. network architecture\n",
    "    3. optimizer\n",
    "    4. loss functioin, binary classification, BCEloss\n",
    "    5. training function\n",
    "    6. evaluation\n",
    "    \n",
    "    \n",
    "jupyter nbconvert --to script step001_prepare_dataset.ipynb    \n",
    "nohup python step001_prepare_dataset.py 1>step001_prepare_dataset.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
