{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os, random, time, copy\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import ndimage, signal\n",
    "from scipy import misc\n",
    "import skimage.transform \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp/step003_architecture\n"
     ]
    }
   ],
   "source": [
    "device ='cpu'\n",
    "supplDevice = 'cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda:1'\n",
    "\n",
    "batch_size = 100\n",
    "total_epoch_num = 200\n",
    "\n",
    "exp_dir = './exp' # experiment directory, used for reading the init model\n",
    "project_name = 'step003_architecture'\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_dir = os.path.join(exp_dir, project_name)\n",
    "print(save_dir)    \n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBattack(Dataset):\n",
    "    def __init__(self, root_dir='DB_trainval.pkl', set_name='train'):\n",
    "        self.root_dir = root_dir\n",
    "        with open(self.root_dir, 'rb') as handle:\n",
    "            self.DB = pickle.load(handle) \n",
    "        #self.itemX = DB['trainX']\n",
    "        #self.itemY = DB['trainY']\n",
    "        self.set_name = set_name # train val/test\n",
    "        self.set_index = {}\n",
    "        self.set_index['train'] = list(range(6637186))\n",
    "        self.set_index['val'] = list(range(6637186, len(self.DB['trainY'])))\n",
    "        self.current_set_len = len(self.set_index[self.set_name])   \n",
    "        \n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.current_set_len\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        curIndex = idx\n",
    "        if self.set_name=='val':\n",
    "            curIndex += 6637186\n",
    "            \n",
    "        X = self.DB['trainX'][curIndex]\n",
    "        Y = self.DB['trainY'][curIndex]\n",
    "        Y = np.asarray([Y]).astype(np.float32)\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        Y = torch.tensor([Y])\n",
    "        return X, Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_datasets = {set_name: \n",
    "                  DBattack(root_dir='DB_trainval.pkl',\n",
    "                           set_name=set_name)\n",
    "                  for set_name in ['train', 'val']}\n",
    "\n",
    "\n",
    "dataloaders = {set_name: DataLoader(whole_datasets[set_name], \n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=set_name=='train', \n",
    "                                    num_workers=2) # num_work can be set to batch_size\n",
    "               for set_name in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {set_name: len(whole_datasets[set_name]) for set_name in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = iter(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlist, ylist = next(sampler)\n",
    "\n",
    "Xlist.shape, ylist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaving blank (backup)\n",
    "\n",
    "    1. dataloader, batch size\n",
    "    2. network architecture\n",
    "    3. optimizer\n",
    "    4. loss functioin, binary classification, BCEloss\n",
    "    5. training function\n",
    "    6. evaluation\n",
    "    \n",
    "    \n",
    "jupyter nbconvert --to script step001_prepare_dataset.ipynb    \n",
    "nohup python step001_prepare_dataset.py 1>step001_prepare_dataset.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
